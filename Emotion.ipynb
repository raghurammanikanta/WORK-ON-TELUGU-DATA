{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lOF_7TDdA8G8"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BpqUAB8r9rTJ",
        "outputId": "55747d98-7ecd-4950-bbe3-e92783dbf718"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting indic-nlp-library\n",
            "  Downloading indic_nlp_library-0.92-py3-none-any.whl.metadata (5.7 kB)\n",
            "Collecting sphinx-argparse (from indic-nlp-library)\n",
            "  Downloading sphinx_argparse-0.5.2-py3-none-any.whl.metadata (3.7 kB)\n",
            "Collecting sphinx-rtd-theme (from indic-nlp-library)\n",
            "  Downloading sphinx_rtd_theme-2.0.0-py2.py3-none-any.whl.metadata (4.4 kB)\n",
            "Collecting morfessor (from indic-nlp-library)\n",
            "  Downloading Morfessor-2.0.6-py3-none-any.whl.metadata (628 bytes)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from indic-nlp-library) (2.1.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from indic-nlp-library) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->indic-nlp-library) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->indic-nlp-library) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->indic-nlp-library) (2024.1)\n",
            "Collecting sphinx>=5.1.0 (from sphinx-argparse->indic-nlp-library)\n",
            "  Downloading sphinx-8.0.2-py3-none-any.whl.metadata (6.2 kB)\n",
            "Collecting docutils>=0.19 (from sphinx-argparse->indic-nlp-library)\n",
            "  Downloading docutils-0.21.2-py3-none-any.whl.metadata (2.8 kB)\n",
            "Collecting sphinx>=5.1.0 (from sphinx-argparse->indic-nlp-library)\n",
            "  Downloading sphinx-7.4.7-py3-none-any.whl.metadata (6.1 kB)\n",
            "Collecting docutils>=0.19 (from sphinx-argparse->indic-nlp-library)\n",
            "  Downloading docutils-0.20.1-py3-none-any.whl.metadata (2.8 kB)\n",
            "Collecting sphinxcontrib-jquery<5,>=4 (from sphinx-rtd-theme->indic-nlp-library)\n",
            "  Downloading sphinxcontrib_jquery-4.1-py2.py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->indic-nlp-library) (1.16.0)\n",
            "Requirement already satisfied: sphinxcontrib-applehelp in /usr/local/lib/python3.10/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2.0.0)\n",
            "Requirement already satisfied: sphinxcontrib-devhelp in /usr/local/lib/python3.10/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2.0.0)\n",
            "Requirement already satisfied: sphinxcontrib-jsmath in /usr/local/lib/python3.10/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (1.0.1)\n",
            "Requirement already satisfied: sphinxcontrib-htmlhelp>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2.1.0)\n",
            "Requirement already satisfied: sphinxcontrib-serializinghtml>=1.1.9 in /usr/local/lib/python3.10/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2.0.0)\n",
            "Requirement already satisfied: sphinxcontrib-qthelp in /usr/local/lib/python3.10/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2.0.0)\n",
            "Requirement already satisfied: Jinja2>=3.1 in /usr/local/lib/python3.10/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (3.1.4)\n",
            "Requirement already satisfied: Pygments>=2.17 in /usr/local/lib/python3.10/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2.18.0)\n",
            "Requirement already satisfied: snowballstemmer>=2.2 in /usr/local/lib/python3.10/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2.2.0)\n",
            "Requirement already satisfied: babel>=2.13 in /usr/local/lib/python3.10/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2.16.0)\n",
            "Requirement already satisfied: alabaster~=0.7.14 in /usr/local/lib/python3.10/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (0.7.16)\n",
            "Requirement already satisfied: imagesize>=1.3 in /usr/local/lib/python3.10/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (1.4.1)\n",
            "Requirement already satisfied: requests>=2.30.0 in /usr/local/lib/python3.10/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2.32.3)\n",
            "Requirement already satisfied: packaging>=23.0 in /usr/local/lib/python3.10/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (24.1)\n",
            "Requirement already satisfied: tomli>=2 in /usr/local/lib/python3.10/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2.0.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from Jinja2>=3.1->sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.30.0->sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.30.0->sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.30.0->sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.30.0->sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2024.8.30)\n",
            "Downloading indic_nlp_library-0.92-py3-none-any.whl (40 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.3/40.3 kB\u001b[0m \u001b[31m348.3 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading Morfessor-2.0.6-py3-none-any.whl (35 kB)\n",
            "Downloading sphinx_argparse-0.5.2-py3-none-any.whl (12 kB)\n",
            "Downloading sphinx_rtd_theme-2.0.0-py2.py3-none-any.whl (2.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading docutils-0.20.1-py3-none-any.whl (572 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m572.7/572.7 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sphinx-7.4.7-py3-none-any.whl (3.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sphinxcontrib_jquery-4.1-py2.py3-none-any.whl (121 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.1/121.1 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: morfessor, docutils, sphinx, sphinxcontrib-jquery, sphinx-argparse, sphinx-rtd-theme, indic-nlp-library\n",
            "  Attempting uninstall: docutils\n",
            "    Found existing installation: docutils 0.18.1\n",
            "    Uninstalling docutils-0.18.1:\n",
            "      Successfully uninstalled docutils-0.18.1\n",
            "  Attempting uninstall: sphinx\n",
            "    Found existing installation: Sphinx 5.0.2\n",
            "    Uninstalling Sphinx-5.0.2:\n",
            "      Successfully uninstalled Sphinx-5.0.2\n",
            "Successfully installed docutils-0.20.1 indic-nlp-library-0.92 morfessor-2.0.6 sphinx-7.4.7 sphinx-argparse-0.5.2 sphinx-rtd-theme-2.0.0 sphinxcontrib-jquery-4.1\n"
          ]
        }
      ],
      "source": [
        "!pip install indic-nlp-library"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import Libraries and Load Dataset"
      ],
      "metadata": {
        "id": "0OzurFnK-gh9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import numpy as np\n",
        "from gensim.models import FastText, Word2Vec\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.preprocessing import MinMaxScaler, MaxAbsScaler\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from indicnlp.tokenize import indic_tokenize\n",
        "\n"
      ],
      "metadata": {
        "id": "Nq9-urKS99eD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Load and preprocess dataset\n",
        "df = pd.read_csv('/content/drive/MyDrive/emotion_filtered.csv')\n",
        "# df.head()\n",
        "# df['topic'] = df['topic'].apply(lambda x: x if x in ['entertainment', 'nation'] else 'others')\n",
        "# c = df['topic'].value_counts()\n",
        "# c\n"
      ],
      "metadata": {
        "id": "rTFSuNt7VKMP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "outputId": "27ea9eae-4fdd-44c3-a233-c48fec486f14"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/drive/MyDrive/emotion_filtered.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-21d9c61bf378>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# # Load and preprocess dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/MyDrive/emotion_filtered.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m# df.head()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# df['topic'] = df['topic'].apply(lambda x: x if x in ['entertainment', 'nation'] else 'others')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# c = df['topic'].value_counts()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    946\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    947\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 948\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    950\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    609\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 611\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    612\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    613\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1446\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1447\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1448\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1449\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1450\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1703\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1704\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1705\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1706\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1707\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    861\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    862\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 863\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    864\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    865\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/emotion_filtered.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()\n",
        "y = df['Emotion']"
      ],
      "metadata": {
        "id": "ttmA0ZKyCa3H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['Emotion'].value_counts()"
      ],
      "metadata": {
        "id": "545sL10OBi24"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Text Preprocessing**"
      ],
      "metadata": {
        "id": "tKBn2pJY_Uap"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def pre_pro(text):\n",
        "    text = re.sub(r'[^ఁ-౿ ]', '', text)\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    return text\n",
        "\n",
        "df['Sentence'] = df['Sentence'].apply(pre_pro)\n",
        "print(\"\\nPreprocessed Text:\")\n",
        "print(df.head())"
      ],
      "metadata": {
        "id": "cLpfAR8L-jI8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "FastText Model Training and Embedding Extraction"
      ],
      "metadata": {
        "id": "k246-fiwV1Gn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['Sentence'] = df['Sentence'].apply(lambda x: x.split() if isinstance(x, str) else x)\n",
        "embedding_size = 100 ##embedding_size is the size of the embedding vector.\n",
        "window_size = 5 #window_size is the size of the number of words\n",
        "min_word = 6 #min_word, which specifies the minimum frequency of a word\n",
        "down_sampling = 1e-2 #most frequently occurring word will be down-sampled by a number specified by the down_sampling attribute\n",
        "\n",
        "\n",
        "ft_model = FastText(sentences = df['Sentence'],vector_size=embedding_size,window=window_size,min_count=min_word,sample=down_sampling,sg=1,epochs=10)\n",
        "#sg defines if the parameter is skpgram or cbow\n",
        "\n",
        "print(ft_model.wv)\n",
        "\n",
        "# Extract word embeddings\n",
        "X = []\n",
        "for sentence in df['Sentence']:\n",
        "    word_embeddings = [ft_model.wv[word] for word in sentence if word in ft_model.wv]\n",
        "    if word_embeddings:\n",
        "        sentence_embedding = np.mean(word_embeddings, axis=0)\n",
        "        X.append(sentence_embedding)\n",
        "    else:\n",
        "        X.append(np.zeros(embedding_size))\n",
        "\n",
        "X = np.array(X)\n"
      ],
      "metadata": {
        "id": "DB-x8nH9U33m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "TF-IDF Embedding Extraction"
      ],
      "metadata": {
        "id": "ARXonffaV9H2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Join the tokenized words back into sentences\n",
        "df['text_joined'] = df['Sentence'].apply(lambda x: ' '.join(x))\n",
        "\n",
        "# Initialize TfidfVectorizer\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "\n",
        "# Fit and transform the text data\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(df['text_joined'])\n",
        "\n",
        "# `tfidf_matrix` now contains your TF-IDF embeddings\n",
        "print(tfidf_matrix)"
      ],
      "metadata": {
        "id": "chhV7POTV3ec"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "TY = tfidf_matrix"
      ],
      "metadata": {
        "id": "7t47VglbmVFd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Word2Vec Model Training and Embedding Extraction"
      ],
      "metadata": {
        "id": "f85snB5EWDtD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from indicnlp.tokenize import indic_tokenize\n",
        "\n",
        "# Assuming df['body'] contains your Telugu text data\n",
        "# df = pd.read_csv('/content/TELUGU_NEWS_TEST.csv')  # Load your Telugu data\n",
        "\n",
        "# Tokenize Telugu sentences\n",
        "sentences = [indic_tokenize.trivial_tokenize(\"\".join(sentence).lower()) for sentence in df['Sentence']]\n",
        "\n",
        "# CBOW model\n",
        "cbow_model = Word2Vec(sentences, vector_size=100, window=5, min_count=1, workers=4, sg=0)\n",
        "\n",
        "# Skip-gram model\n",
        "sg_model = Word2Vec(sentences, vector_size=100, window=5, min_count=1, workers=4, sg=1)\n",
        "\n",
        "# Save the models\n",
        "cbow_model.save(\"telugu_word2vec_cbow.model\")\n",
        "sg_model.save(\"telugu_word2vec_sg.model\")\n",
        "w2v_model = Word2Vec.load(\"telugu_word2vec_cbow.model\")\n",
        "\n",
        "def get_document_vector(doc):\n",
        "    # Check if doc is a list and join it into a string if necessary\n",
        "    if isinstance(doc, list):\n",
        "        doc = \" \".join(doc)\n",
        "    words = doc.split()\n",
        "    word_vecs = [w2v_model.wv[word] for word in words if word in w2v_model.wv]\n",
        "    if len(word_vecs) == 0:\n",
        "        return np.zeros(w2v_model.vector_size)\n",
        "    return np.mean(word_vecs, axis=0)\n",
        "\n",
        "# Create document vectors\n",
        "X_w2v = np.array([get_document_vector(doc) for doc in df['Sentence']])\n",
        "yw = df['Emotion']"
      ],
      "metadata": {
        "id": "cH9dqPKvWG2i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " Train-Test Split"
      ],
      "metadata": {
        "id": "Q2XI2RJTWN3e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the data\n",
        "WX_train, WX_test, Wy_train, Wy_test = train_test_split(X_w2v, yw, test_size=0.2, random_state=42)\n",
        "TX_train, TX_test, Ty_train, Ty_test = train_test_split(TY, y, test_size=0.2, random_state=42)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
      ],
      "metadata": {
        "id": "rbq-kSWpWOh-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " SVM Model Training and Evaluation"
      ],
      "metadata": {
        "id": "Jl_FisHpWfc9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# SVM with TF-IDF\n",
        "clf = SVC(kernel='linear')  # You can explore other kernels as well\n",
        "clf.fit(TX_train, Ty_train)\n",
        "Ty_pred = clf.predict(TX_test)\n",
        "print(f'\\nSVM with TF-IDF Accuracy: {accuracy_score(Ty_test, Ty_pred)}')\n",
        "print(classification_report(Ty_test, Ty_pred))\n",
        "\n",
        "# SVM with FastText\n",
        "clf.fit(X_train, y_train)\n",
        "y_pred = clf.predict(X_test)\n",
        "print(f'\\nSVM with FastText Accuracy: {accuracy_score(y_test, y_pred)}')\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "# SVM with Word2Vec\n",
        "svm_model = SVC(kernel='linear')\n",
        "svm_model.fit(WX_train, Wy_train)\n",
        "yw_pred = svm_model.predict(WX_test)\n",
        "print(f'\\nSVM with Word2Vec Accuracy: {accuracy_score(Wy_test, y_pred)}')\n",
        "print(classification_report(Wy_test, yw_pred))\n"
      ],
      "metadata": {
        "id": "EvH_yQ4RWgKt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "KNN Model Training and Evaluation"
      ],
      "metadata": {
        "id": "z2EDdTgXWk-w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# KNN with FastText\n",
        "knn_classifier = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_classifier.fit(X_train, y_train)\n",
        "y_pred = knn_classifier.predict(X_test)\n",
        "print(f'\\nKNN with FastText Accuracy: {accuracy_score(y_test, y_pred)}')\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "# KNN with TF-IDF\n",
        "knn_classifier.fit(TX_train, Ty_train)\n",
        "Ty_pred = knn_classifier.predict(TX_test)\n",
        "print(f'\\nKNN with TF-IDF Accuracy: {accuracy_score(Ty_test, Ty_pred)}')\n",
        "print(classification_report(Ty_test, Ty_pred))\n",
        "\n",
        "# KNN with Word2Vec\n",
        "knn_classifier.fit(WX_train, Wy_train)\n",
        "y_pred = knn_classifier.predict(WX_test)\n",
        "print(f'\\nKNN with Word2Vec Accuracy: {accuracy_score(Wy_test, y_pred)}')\n",
        "print(classification_report(Wy_test, y_pred))\n"
      ],
      "metadata": {
        "id": "WeQAtUAKWnFS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Logistic Regression Model Training and Evaluation"
      ],
      "metadata": {
        "id": "F7E3zoihWrVO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Logistic Regression with FastText\n",
        "logreg_classifier = LogisticRegression()\n",
        "logreg_classifier.fit(X_train, y_train)\n",
        "y_pred = logreg_classifier.predict(X_test)\n",
        "print(f'\\nLogistic Regression with FastText Accuracy: {accuracy_score(y_test, y_pred)}')\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "# Logistic Regression with TF-IDF\n",
        "logreg_classifier.fit(TX_train, Ty_train)\n",
        "Ty_pred = logreg_classifier.predict(TX_test)\n",
        "print(f'\\nLogistic Regression with TF-IDF Accuracy: {accuracy_score(Ty_test, Ty_pred)}')\n",
        "print(classification_report(Ty_test, Ty_pred))\n",
        "\n",
        "# Logistic Regression with Word2Vec\n",
        "logreg_classifier.fit(WX_train, Wy_train)\n",
        "Wy_pred = logreg_classifier.predict(WX_test)\n",
        "print(f'\\nLogistic Regression with Word2Vec Accuracy: {accuracy_score(Wy_test, Wy_pred)}')\n",
        "print(classification_report(Wy_test, Wy_pred))\n"
      ],
      "metadata": {
        "id": "F1Z2oP4TWtFE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Decision Tree Model Training and Evaluation"
      ],
      "metadata": {
        "id": "P_nUS1CcXBJP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Decision Tree with FastText\n",
        "dt_classifier = DecisionTreeClassifier()\n",
        "dt_classifier.fit(X_train, y_train)\n",
        "y_pred = dt_classifier.predict(X_test)\n",
        "print(f'\\nDecision Tree with FastText Accuracy: {accuracy_score(y_test, y_pred)}')\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "# Decision Tree with TF-IDF\n",
        "dt_classifier.fit(TX_train, Ty_train)\n",
        "Ty_pred = dt_classifier.predict(TX_test)\n",
        "print(f'\\nDecision Tree with TF-IDF Accuracy: {accuracy_score(Ty_test, Ty_pred)}')\n",
        "print(classification_report(Ty_test, Ty_pred))\n",
        "\n",
        "# Decision Tree with Word2Vec\n",
        "dt_classifier.fit(WX_train, Wy_train)\n",
        "Wy_pred = dt_classifier.predict(WX_test)\n",
        "print(f'\\nDecision Tree with Word2Vec Accuracy: {accuracy_score(Wy_test, Wy_pred)}')\n",
        "print(classification_report(Wy_test, Wy_pred))\n"
      ],
      "metadata": {
        "id": "_voeNZlPXBwe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Naive Bayes Model Training and Evaluation"
      ],
      "metadata": {
        "id": "_Ca__ODGXFzH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Naive Bayes with FastText\n",
        "scaler = MinMaxScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "nb_classifier = MultinomialNB()\n",
        "nb_classifier.fit(X_train_scaled, y_train)\n",
        "y_pred = nb_classifier.predict(X_test_scaled)\n",
        "print(f'\\nNaive Bayes with FastText Accuracy: {accuracy_score(y_test, y_pred)}')\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "# Naive Bayes with TF-IDF\n",
        "scaler = MaxAbsScaler()\n",
        "X_train_scaled = scaler.fit_transform(TX_train)\n",
        "X_test_scaled = scaler.transform(TX_test)\n",
        "nb_classifier.fit(X_train_scaled, Ty_train)\n",
        "Ty_pred = nb_classifier.predict(X_test_scaled)\n",
        "print(f'\\nNaive Bayes with TF-IDF Accuracy: {accuracy_score(Ty_test, Ty_pred)}')\n",
        "print(classification_report(Ty_test, Ty_pred))\n",
        "\n",
        "# Naive Bayes with Word2Vec\n",
        "scaler = MinMaxScaler()\n",
        "WX_train_scaled = scaler.fit_transform(WX_train)\n",
        "WX_test_scaled = scaler.transform(WX_test)\n",
        "nb_classifier.fit(WX_train_scaled, Wy_train)\n",
        "Wy_pred = nb_classifier.predict(WX_test_scaled)\n",
        "print(f'\\nNaive Bayes with Word2Vec Accuracy: {accuracy_score(Wy_test, Wy_pred)}')\n",
        "print(classification_report(Wy_test, Wy_pred))\n"
      ],
      "metadata": {
        "id": "jsPdTsWWXIci"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Random Forest Model Training and Evaluation"
      ],
      "metadata": {
        "id": "axApkQYSXMVp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Random Forest with FastText\n",
        "rf_classifier = RandomForestClassifier(n_estimators=100)\n",
        "rf_classifier.fit(X_train, y_train)\n",
        "y_pred = rf_classifier.predict(X_test)\n",
        "print(f'\\nRandom Forest with FastText Accuracy: {accuracy_score(y_test, y_pred)}')\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "# Random Forest with TF-IDF\n",
        "rf_classifier.fit(TX_train, Ty_train)\n",
        "Ty_pred = rf_classifier.predict(TX_test)\n",
        "print(f'\\nRandom Forest with TF-IDF Accuracy: {accuracy_score(Ty_test, Ty_pred)}')\n",
        "print(classification_report(Ty_test, Ty_pred))\n",
        "\n",
        "# Random Forest with Word2Vec\n",
        "rf_classifier.fit(WX_train, Wy_train)\n",
        "Wy_pred = rf_classifier.predict(WX_test)\n",
        "print(f'\\nRandom Forest with Word2Vec Accuracy: {accuracy_score(Wy_test, Wy_pred)}')\n",
        "print(classification_report(Wy_test, Wy_pred))\n"
      ],
      "metadata": {
        "id": "PLAFFq2qXO_i"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}