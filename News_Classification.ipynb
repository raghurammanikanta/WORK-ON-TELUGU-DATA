{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNk+UKFtT/eiLOIH+j4W1pB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/raghurammanikanta/WORK-ON-TELUGU-DATA/blob/main/complete_work_on_telugu_news_test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BpqUAB8r9rTJ",
        "outputId": "7d9177cd-4768-4227-a653-71f32602a68c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: indic-nlp-library in /usr/local/lib/python3.10/dist-packages (0.92)\n",
            "Requirement already satisfied: sphinx-argparse in /usr/local/lib/python3.10/dist-packages (from indic-nlp-library) (0.5.2)\n",
            "Requirement already satisfied: sphinx-rtd-theme in /usr/local/lib/python3.10/dist-packages (from indic-nlp-library) (2.0.0)\n",
            "Requirement already satisfied: morfessor in /usr/local/lib/python3.10/dist-packages (from indic-nlp-library) (2.0.6)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from indic-nlp-library) (2.0.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from indic-nlp-library) (1.25.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->indic-nlp-library) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->indic-nlp-library) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->indic-nlp-library) (2024.1)\n",
            "Requirement already satisfied: sphinx>=5.1.0 in /usr/local/lib/python3.10/dist-packages (from sphinx-argparse->indic-nlp-library) (7.4.7)\n",
            "Requirement already satisfied: docutils>=0.19 in /usr/local/lib/python3.10/dist-packages (from sphinx-argparse->indic-nlp-library) (0.20.1)\n",
            "Requirement already satisfied: sphinxcontrib-jquery<5,>=4 in /usr/local/lib/python3.10/dist-packages (from sphinx-rtd-theme->indic-nlp-library) (4.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->indic-nlp-library) (1.16.0)\n",
            "Requirement already satisfied: sphinxcontrib-applehelp in /usr/local/lib/python3.10/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (1.0.8)\n",
            "Requirement already satisfied: sphinxcontrib-devhelp in /usr/local/lib/python3.10/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (1.0.6)\n",
            "Requirement already satisfied: sphinxcontrib-jsmath in /usr/local/lib/python3.10/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (1.0.1)\n",
            "Requirement already satisfied: sphinxcontrib-htmlhelp>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2.0.6)\n",
            "Requirement already satisfied: sphinxcontrib-serializinghtml>=1.1.9 in /usr/local/lib/python3.10/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (1.1.10)\n",
            "Requirement already satisfied: sphinxcontrib-qthelp in /usr/local/lib/python3.10/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (1.0.8)\n",
            "Requirement already satisfied: Jinja2>=3.1 in /usr/local/lib/python3.10/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (3.1.4)\n",
            "Requirement already satisfied: Pygments>=2.17 in /usr/local/lib/python3.10/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2.18.0)\n",
            "Requirement already satisfied: snowballstemmer>=2.2 in /usr/local/lib/python3.10/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2.2.0)\n",
            "Requirement already satisfied: babel>=2.13 in /usr/local/lib/python3.10/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2.15.0)\n",
            "Requirement already satisfied: alabaster~=0.7.14 in /usr/local/lib/python3.10/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (0.7.16)\n",
            "Requirement already satisfied: imagesize>=1.3 in /usr/local/lib/python3.10/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (1.4.1)\n",
            "Requirement already satisfied: requests>=2.30.0 in /usr/local/lib/python3.10/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2.31.0)\n",
            "Requirement already satisfied: packaging>=23.0 in /usr/local/lib/python3.10/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (24.1)\n",
            "Requirement already satisfied: tomli>=2 in /usr/local/lib/python3.10/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2.0.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from Jinja2>=3.1->sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.30.0->sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.30.0->sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.30.0->sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.30.0->sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2024.7.4)\n"
          ]
        }
      ],
      "source": [
        "!pip install indic-nlp-library"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import Libraries and Load Dataset"
      ],
      "metadata": {
        "id": "0OzurFnK-gh9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import numpy as np\n",
        "from gensim.models import FastText, Word2Vec\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.preprocessing import MinMaxScaler, MaxAbsScaler\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from indicnlp.tokenize import indic_tokenize\n",
        "\n"
      ],
      "metadata": {
        "id": "Nq9-urKS99eD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and preprocess dataset\n",
        "df = pd.read_csv('/content/TELUGU_NEWS_TEST.csv')\n",
        "df.head()\n",
        "df['topic'] = df['topic'].apply(lambda x: x if x in ['entertainment', 'nation'] else 'others')\n",
        "c = df['topic'].value_counts()\n",
        "c\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rTFSuNt7VKMP",
        "outputId": "cb2fca3a-24ef-4a2e-ea25-1a05e4d8976e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "topic\n",
              "nation           1673\n",
              "others           1367\n",
              "entertainment    1289\n",
              "Name: count, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Text Preprocessing**"
      ],
      "metadata": {
        "id": "tKBn2pJY_Uap"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def pre_pro(text):\n",
        "    text = re.sub(r'[^ఁ-౿ ]', '', text)\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    return text\n",
        "\n",
        "df['body'] = df['body'].apply(pre_pro)\n",
        "print(\"\\nPreprocessed Text:\")\n",
        "print(df.head())"
      ],
      "metadata": {
        "id": "cLpfAR8L-jI8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d3df25a0-1ca3-449e-98b2-15d25a4bdff5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Preprocessed Text:\n",
            "     SNo                 date  \\\n",
            "0   7771  19-05-2017 13:44:10   \n",
            "1   9591  01-08-2017 13:58:24   \n",
            "2  12622  05-04-2017 00:31:58   \n",
            "3  16401  30-08-2017 03:34:37   \n",
            "4  10422  10-04-2017 09:00:48   \n",
            "\n",
            "                                             heading  \\\n",
            "0                                               కేశవ   \n",
            "1  డబ్బుల కోసం ప్రాణాల మీదకు తెచ్చుకుంటారా.. హీరో...   \n",
            "2                    దక్షిణాదిన బీజేపీ పప్పులుడకవ్‌    \n",
            "3                 పీఎస్‌ఎల్వీ-సీ39కి గ్రీన్‌సిగ్నల్‌   \n",
            "4  ఏయ్ జయప్రకాష్ అనే కేక వినిపించింది.. వెనక్కి త...   \n",
            "\n",
            "                                                body          topic  \n",
            "0  హీరోగా తెలుగు సినిమాల్లోకి ఎంట్రీ ఇచ్చిన నిఖిల...  entertainment  \n",
            "1  సినిమాల్లో యాక్షన్ స్టంట్లు చేసేటప్పుడు ఎక్కువ...  entertainment  \n",
            "2  దక్షిణ భారతాన్ని ఆక్రమించేందుకు బీజేపీ పంచెలు ...         nation  \n",
            "3  నేటి మధ్యాహ్నం కి కౌంట్డౌన్ షురూశ్రీహరికోట ఆగస...         nation  \n",
            "4  ఏమి రా అబ్బి యాడికి పోయినావు అంటూ రాయలసీమ యాసన...  entertainment  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "FastText Model Training and Embedding Extraction"
      ],
      "metadata": {
        "id": "k246-fiwV1Gn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['body'] = df['body'].apply(lambda x: x.split())\n",
        "embedding_size = 100 ##embedding_size is the size of the embedding vector.\n",
        "window_size = 5 #window_size is the size of the number of words\n",
        "min_word = 6 #min_word, which specifies the minimum frequency of a word\n",
        "down_sampling = 1e-2 #most frequently occurring word will be down-sampled by a number specified by the down_sampling attribute\n",
        "\n",
        "\n",
        "ft_model = FastText(sentences = df['body'],vector_size=embedding_size,window=window_size,min_count=min_word,sample=down_sampling,sg=1,epochs=10)\n",
        "#sg defines if the parameter is skpgram or cbow\n",
        "\n",
        "print(ft_model.wv)\n",
        "\n",
        "# Extract word embeddings\n",
        "X = []\n",
        "for sentence in df['body']:\n",
        "    word_embeddings = [ft_model.wv[word] for word in sentence if word in ft_model.wv]\n",
        "    if word_embeddings:\n",
        "        sentence_embedding = np.mean(word_embeddings, axis=0)\n",
        "        X.append(sentence_embedding)\n",
        "    else:\n",
        "        X.append(np.zeros(embedding_size))\n",
        "\n",
        "X = np.array(X)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DB-x8nH9U33m",
        "outputId": "e08505f1-cac3-48d2-a2f7-c84d5686b407"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FastTextKeyedVectors<vector_size=100, 16282 keys>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "TF-IDF Embedding Extraction"
      ],
      "metadata": {
        "id": "ARXonffaV9H2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Join the tokenized words back into sentences\n",
        "df['text_joined'] = df['body'].apply(lambda x: ' '.join(x))\n",
        "\n",
        "# Initialize TfidfVectorizer\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "\n",
        "# Fit and transform the text data\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(df['text_joined'])\n",
        "\n",
        "# `tfidf_matrix` now contains your TF-IDF embeddings\n",
        "print(tfidf_matrix)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "chhV7POTV3ec",
        "outputId": "48a8744d-b554-49a7-f9ee-fce12ae3c380"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  (0, 5168)\t0.06070476226620654\n",
            "  (0, 3607)\t0.05400373939202602\n",
            "  (0, 2224)\t0.03709842715448317\n",
            "  (0, 1030)\t0.05511371558487567\n",
            "  (0, 746)\t0.04965128960888237\n",
            "  (0, 4717)\t0.03340671972725232\n",
            "  (0, 2535)\t0.025061712619676796\n",
            "  (0, 4646)\t0.05436043153595191\n",
            "  (0, 1176)\t0.05253748783705576\n",
            "  (0, 2248)\t0.05400373939202602\n",
            "  (0, 1225)\t0.07891742938490594\n",
            "  (0, 3217)\t0.0684516743401461\n",
            "  (0, 3209)\t0.03491484879890116\n",
            "  (0, 1099)\t0.03978189929167606\n",
            "  (0, 1742)\t0.034685125687291596\n",
            "  (0, 2651)\t0.021518894057083474\n",
            "  (0, 3584)\t0.03490522998343258\n",
            "  (0, 4403)\t0.021001460102973747\n",
            "  (0, 4450)\t0.04126980087199144\n",
            "  (0, 997)\t0.06534756666109966\n",
            "  (0, 4727)\t0.061302808200556266\n",
            "  (0, 461)\t0.028177443722581918\n",
            "  (0, 4339)\t0.028643730534997587\n",
            "  (0, 1202)\t0.02149739665804461\n",
            "  (0, 2520)\t0.026223425569685106\n",
            "  :\t:\n",
            "  (4328, 3720)\t0.10193484367823911\n",
            "  (4328, 800)\t0.07924725908723386\n",
            "  (4328, 3953)\t0.0595326739373003\n",
            "  (4328, 1280)\t0.027572565385999184\n",
            "  (4328, 392)\t0.05724895167842428\n",
            "  (4328, 5629)\t0.04132315436796297\n",
            "  (4328, 2677)\t0.026514912151942373\n",
            "  (4328, 4744)\t0.06652772125008723\n",
            "  (4328, 4666)\t0.027912072423388487\n",
            "  (4328, 4823)\t0.2029565223395679\n",
            "  (4328, 1848)\t0.10092951508842964\n",
            "  (4328, 115)\t0.07311002397724704\n",
            "  (4328, 134)\t0.028458306377079735\n",
            "  (4328, 760)\t0.03483050381899106\n",
            "  (4328, 4902)\t0.06246627470948205\n",
            "  (4328, 2899)\t0.024385760928315292\n",
            "  (4328, 74)\t0.19236711768227288\n",
            "  (4328, 3337)\t0.148881390970939\n",
            "  (4328, 4048)\t0.025722060339173205\n",
            "  (4328, 4889)\t0.1302721670355032\n",
            "  (4328, 1128)\t0.039401441045003226\n",
            "  (4328, 722)\t0.11831471661388446\n",
            "  (4328, 5341)\t0.1750286145456594\n",
            "  (4328, 2954)\t0.11874079014982139\n",
            "  (4328, 470)\t0.03941982838650248\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "TY = tfidf_matrix"
      ],
      "metadata": {
        "id": "7t47VglbmVFd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Word2Vec Model Training and Embedding Extraction"
      ],
      "metadata": {
        "id": "f85snB5EWDtD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from indicnlp.tokenize import indic_tokenize\n",
        "\n",
        "# Assuming df['body'] contains your Telugu text data\n",
        "df = pd.read_csv('/content/TELUGU_NEWS_TEST.csv')  # Load your Telugu data\n",
        "\n",
        "# Tokenize Telugu sentences\n",
        "sentences = [indic_tokenize.trivial_tokenize(sentence.lower()) for sentence in df['body']]\n",
        "\n",
        "# CBOW model\n",
        "cbow_model = Word2Vec(sentences, vector_size=100, window=5, min_count=1, workers=4, sg=0)\n",
        "\n",
        "# Skip-gram model\n",
        "sg_model = Word2Vec(sentences, vector_size=100, window=5, min_count=1, workers=4, sg=1)\n",
        "\n",
        "# Save the models\n",
        "cbow_model.save(\"telugu_word2vec_cbow.model\")\n",
        "sg_model.save(\"telugu_word2vec_sg.model\")\n",
        "w2v_model = Word2Vec.load(\"telugu_word2vec_cbow.model\")\n",
        "\n",
        "# Function to get document vector\n",
        "def get_document_vector(doc):\n",
        "    words = doc.split()\n",
        "    word_vecs = [w2v_model.wv[word] for word in words if word in w2v_model.wv]\n",
        "    if len(word_vecs) == 0:\n",
        "        return np.zeros(w2v_model.vector_size)\n",
        "    return np.mean(word_vecs, axis=0)\n",
        "\n",
        "# Create document vectors\n",
        "X_w2v = np.array([get_document_vector(doc) for doc in df['body']])\n",
        "yw = df['topic']"
      ],
      "metadata": {
        "id": "cH9dqPKvWG2i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " Train-Test Split"
      ],
      "metadata": {
        "id": "Q2XI2RJTWN3e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the data\n",
        "WX_train, WX_test, Wy_train, Wy_test = train_test_split(X_w2v, yw, test_size=0.2, random_state=42)\n",
        "TX_train, TX_test, Ty_train, Ty_test = train_test_split(TY, y, test_size=0.2, random_state=42)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
      ],
      "metadata": {
        "id": "rbq-kSWpWOh-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " SVM Model Training and Evaluation"
      ],
      "metadata": {
        "id": "Jl_FisHpWfc9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# SVM with TF-IDF\n",
        "clf = SVC(kernel='linear')  # You can explore other kernels as well\n",
        "clf.fit(TX_train, Ty_train)\n",
        "Ty_pred = clf.predict(TX_test)\n",
        "print(f'\\nSVM with TF-IDF Accuracy: {accuracy_score(Ty_test, Ty_pred)}')\n",
        "print(classification_report(Ty_test, Ty_pred))\n",
        "\n",
        "# SVM with FastText\n",
        "clf.fit(X_train, y_train)\n",
        "y_pred = clf.predict(X_test)\n",
        "print(f'\\nSVM with FastText Accuracy: {accuracy_score(y_test, y_pred)}')\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "# SVM with Word2Vec\n",
        "svm_model = SVC(kernel='linear')\n",
        "svm_model.fit(WX_train, Wy_train)\n",
        "yw_pred = svm_model.predict(WX_test)\n",
        "print(f'\\nSVM with Word2Vec Accuracy: {accuracy_score(Wy_test, y_pred)}')\n",
        "print(classification_report(Wy_test, yw_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EvH_yQ4RWgKt",
        "outputId": "9d2ba950-0e7b-4a17-b460-4b6e2f756ec6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "SVM with TF-IDF Accuracy: 0.8602771362586605\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "entertainment       0.94      0.90      0.92       237\n",
            "       nation       0.85      0.86      0.85       357\n",
            "       others       0.81      0.83      0.82       272\n",
            "\n",
            "     accuracy                           0.86       866\n",
            "    macro avg       0.87      0.86      0.86       866\n",
            " weighted avg       0.86      0.86      0.86       866\n",
            "\n",
            "\n",
            "SVM with FastText Accuracy: 0.9122401847575058\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "entertainment       0.98      0.96      0.97       237\n",
            "       nation       0.92      0.87      0.90       357\n",
            "       others       0.85      0.92      0.88       272\n",
            "\n",
            "     accuracy                           0.91       866\n",
            "    macro avg       0.92      0.92      0.92       866\n",
            " weighted avg       0.91      0.91      0.91       866\n",
            "\n",
            "\n",
            "SVM with Word2Vec Accuracy: 0.6235565819861432\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "     business       0.70      0.63      0.66       133\n",
            "    editorial       0.45      0.17      0.25        58\n",
            "entertainment       0.95      0.91      0.93       237\n",
            "       nation       0.71      0.87      0.78       357\n",
            "       sports       0.83      0.59      0.69        81\n",
            "\n",
            "     accuracy                           0.77       866\n",
            "    macro avg       0.73      0.64      0.66       866\n",
            " weighted avg       0.77      0.77      0.76       866\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "KNN Model Training and Evaluation"
      ],
      "metadata": {
        "id": "z2EDdTgXWk-w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# KNN with FastText\n",
        "knn_classifier = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_classifier.fit(X_train, y_train)\n",
        "y_pred = knn_classifier.predict(X_test)\n",
        "print(f'\\nKNN with FastText Accuracy: {accuracy_score(y_test, y_pred)}')\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "# KNN with TF-IDF\n",
        "knn_classifier.fit(TX_train, Ty_train)\n",
        "Ty_pred = knn_classifier.predict(TX_test)\n",
        "print(f'\\nKNN with TF-IDF Accuracy: {accuracy_score(Ty_test, Ty_pred)}')\n",
        "print(classification_report(Ty_test, Ty_pred))\n",
        "\n",
        "# KNN with Word2Vec\n",
        "knn_classifier.fit(WX_train, Wy_train)\n",
        "y_pred = knn_classifier.predict(WX_test)\n",
        "print(f'\\nKNN with Word2Vec Accuracy: {accuracy_score(Wy_test, y_pred)}')\n",
        "print(classification_report(Wy_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WeQAtUAKWnFS",
        "outputId": "2a0ba617-fe32-40fb-d29b-1ad83df92be2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "KNN with FastText Accuracy: 0.9260969976905312\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "entertainment       0.99      0.96      0.98       237\n",
            "       nation       0.93      0.90      0.91       357\n",
            "       others       0.87      0.93      0.90       272\n",
            "\n",
            "     accuracy                           0.93       866\n",
            "    macro avg       0.93      0.93      0.93       866\n",
            " weighted avg       0.93      0.93      0.93       866\n",
            "\n",
            "\n",
            "KNN with TF-IDF Accuracy: 0.3556581986143187\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "entertainment       0.95      0.08      0.15       237\n",
            "       nation       0.90      0.05      0.10       357\n",
            "       others       0.33      1.00      0.49       272\n",
            "\n",
            "     accuracy                           0.36       866\n",
            "    macro avg       0.73      0.38      0.25       866\n",
            " weighted avg       0.73      0.36      0.23       866\n",
            "\n",
            "\n",
            "KNN with Word2Vec Accuracy: 0.7575057736720554\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "     business       0.62      0.68      0.65       133\n",
            "    editorial       0.53      0.57      0.55        58\n",
            "entertainment       0.91      0.89      0.90       237\n",
            "       nation       0.76      0.76      0.76       357\n",
            "       sports       0.71      0.60      0.65        81\n",
            "\n",
            "     accuracy                           0.76       866\n",
            "    macro avg       0.71      0.70      0.70       866\n",
            " weighted avg       0.76      0.76      0.76       866\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Logistic Regression Model Training and Evaluation"
      ],
      "metadata": {
        "id": "F7E3zoihWrVO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Logistic Regression with FastText\n",
        "logreg_classifier = LogisticRegression()\n",
        "logreg_classifier.fit(X_train, y_train)\n",
        "y_pred = logreg_classifier.predict(X_test)\n",
        "print(f'\\nLogistic Regression with FastText Accuracy: {accuracy_score(y_test, y_pred)}')\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "# Logistic Regression with TF-IDF\n",
        "logreg_classifier.fit(TX_train, Ty_train)\n",
        "Ty_pred = logreg_classifier.predict(TX_test)\n",
        "print(f'\\nLogistic Regression with TF-IDF Accuracy: {accuracy_score(Ty_test, Ty_pred)}')\n",
        "print(classification_report(Ty_test, Ty_pred))\n",
        "\n",
        "# Logistic Regression with Word2Vec\n",
        "logreg_classifier.fit(WX_train, Wy_train)\n",
        "Wy_pred = logreg_classifier.predict(WX_test)\n",
        "print(f'\\nLogistic Regression with Word2Vec Accuracy: {accuracy_score(Wy_test, Wy_pred)}')\n",
        "print(classification_report(Wy_test, Wy_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F1Z2oP4TWtFE",
        "outputId": "fa40a306-74d9-41ef-d915-8f5d281ae2e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Logistic Regression with FastText Accuracy: 0.9064665127020786\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "entertainment       0.98      0.95      0.97       237\n",
            "       nation       0.91      0.88      0.89       357\n",
            "       others       0.84      0.90      0.87       272\n",
            "\n",
            "     accuracy                           0.91       866\n",
            "    macro avg       0.91      0.91      0.91       866\n",
            " weighted avg       0.91      0.91      0.91       866\n",
            "\n",
            "\n",
            "Logistic Regression with TF-IDF Accuracy: 0.8614318706697459\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "entertainment       0.94      0.92      0.93       237\n",
            "       nation       0.85      0.86      0.86       357\n",
            "       others       0.81      0.81      0.81       272\n",
            "\n",
            "     accuracy                           0.86       866\n",
            "    macro avg       0.87      0.86      0.87       866\n",
            " weighted avg       0.86      0.86      0.86       866\n",
            "\n",
            "\n",
            "Logistic Regression with Word2Vec Accuracy: 0.766743648960739\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "     business       0.70      0.66      0.68       133\n",
            "    editorial       0.37      0.17      0.24        58\n",
            "entertainment       0.94      0.92      0.93       237\n",
            "       nation       0.72      0.85      0.78       357\n",
            "       sports       0.77      0.54      0.64        81\n",
            "\n",
            "     accuracy                           0.77       866\n",
            "    macro avg       0.70      0.63      0.65       866\n",
            " weighted avg       0.76      0.77      0.75       866\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Decision Tree Model Training and Evaluation"
      ],
      "metadata": {
        "id": "P_nUS1CcXBJP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Decision Tree with FastText\n",
        "dt_classifier = DecisionTreeClassifier()\n",
        "dt_classifier.fit(X_train, y_train)\n",
        "y_pred = dt_classifier.predict(X_test)\n",
        "print(f'\\nDecision Tree with FastText Accuracy: {accuracy_score(y_test, y_pred)}')\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "# Decision Tree with TF-IDF\n",
        "dt_classifier.fit(TX_train, Ty_train)\n",
        "Ty_pred = dt_classifier.predict(TX_test)\n",
        "print(f'\\nDecision Tree with TF-IDF Accuracy: {accuracy_score(Ty_test, Ty_pred)}')\n",
        "print(classification_report(Ty_test, Ty_pred))\n",
        "\n",
        "# Decision Tree with Word2Vec\n",
        "dt_classifier.fit(WX_train, Wy_train)\n",
        "Wy_pred = dt_classifier.predict(WX_test)\n",
        "print(f'\\nDecision Tree with Word2Vec Accuracy: {accuracy_score(Wy_test, Wy_pred)}')\n",
        "print(classification_report(Wy_test, Wy_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_voeNZlPXBwe",
        "outputId": "92933bb2-a767-47a1-964b-d449ab102f07"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Decision Tree with FastText Accuracy: 0.8314087759815243\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "entertainment       0.92      0.89      0.91       237\n",
            "       nation       0.82      0.80      0.81       357\n",
            "       others       0.77      0.82      0.79       272\n",
            "\n",
            "     accuracy                           0.83       866\n",
            "    macro avg       0.84      0.84      0.84       866\n",
            " weighted avg       0.83      0.83      0.83       866\n",
            "\n",
            "\n",
            "Decision Tree with TF-IDF Accuracy: 0.6212471131639723\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "entertainment       0.69      0.73      0.71       237\n",
            "       nation       0.63      0.58      0.60       357\n",
            "       others       0.55      0.58      0.57       272\n",
            "\n",
            "     accuracy                           0.62       866\n",
            "    macro avg       0.62      0.63      0.63       866\n",
            " weighted avg       0.62      0.62      0.62       866\n",
            "\n",
            "\n",
            "Decision Tree with Word2Vec Accuracy: 0.7032332563510393\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "     business       0.58      0.56      0.57       133\n",
            "    editorial       0.43      0.34      0.38        58\n",
            "entertainment       0.87      0.85      0.86       237\n",
            "       nation       0.73      0.74      0.73       357\n",
            "       sports       0.51      0.63      0.56        81\n",
            "\n",
            "     accuracy                           0.70       866\n",
            "    macro avg       0.62      0.62      0.62       866\n",
            " weighted avg       0.70      0.70      0.70       866\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Naive Bayes Model Training and Evaluation"
      ],
      "metadata": {
        "id": "_Ca__ODGXFzH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Naive Bayes with FastText\n",
        "scaler = MinMaxScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "nb_classifier = MultinomialNB()\n",
        "nb_classifier.fit(X_train_scaled, y_train)\n",
        "y_pred = nb_classifier.predict(X_test_scaled)\n",
        "print(f'\\nNaive Bayes with FastText Accuracy: {accuracy_score(y_test, y_pred)}')\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "# Naive Bayes with TF-IDF\n",
        "scaler = MaxAbsScaler()\n",
        "X_train_scaled = scaler.fit_transform(TX_train)\n",
        "X_test_scaled = scaler.transform(TX_test)\n",
        "nb_classifier.fit(X_train_scaled, Ty_train)\n",
        "Ty_pred = nb_classifier.predict(X_test_scaled)\n",
        "print(f'\\nNaive Bayes with TF-IDF Accuracy: {accuracy_score(Ty_test, Ty_pred)}')\n",
        "print(classification_report(Ty_test, Ty_pred))\n",
        "\n",
        "# Naive Bayes with Word2Vec\n",
        "scaler = MinMaxScaler()\n",
        "WX_train_scaled = scaler.fit_transform(WX_train)\n",
        "WX_test_scaled = scaler.transform(WX_test)\n",
        "nb_classifier.fit(WX_train_scaled, Wy_train)\n",
        "Wy_pred = nb_classifier.predict(WX_test_scaled)\n",
        "print(f'\\nNaive Bayes with Word2Vec Accuracy: {accuracy_score(Wy_test, Wy_pred)}')\n",
        "print(classification_report(Wy_test, Wy_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jsPdTsWWXIci",
        "outputId": "c7e9e531-464f-41c8-abc3-c24dc1682112"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Naive Bayes with FastText Accuracy: 0.8452655889145496\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "entertainment       0.98      0.90      0.94       237\n",
            "       nation       0.76      0.95      0.84       357\n",
            "       others       0.90      0.66      0.76       272\n",
            "\n",
            "     accuracy                           0.85       866\n",
            "    macro avg       0.88      0.84      0.85       866\n",
            " weighted avg       0.86      0.85      0.84       866\n",
            "\n",
            "\n",
            "Naive Bayes with TF-IDF Accuracy: 0.8302540415704388\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "entertainment       0.92      0.90      0.91       237\n",
            "       nation       0.80      0.85      0.82       357\n",
            "       others       0.80      0.74      0.77       272\n",
            "\n",
            "     accuracy                           0.83       866\n",
            "    macro avg       0.84      0.83      0.83       866\n",
            " weighted avg       0.83      0.83      0.83       866\n",
            "\n",
            "\n",
            "Naive Bayes with Word2Vec Accuracy: 0.6570438799076213\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "     business       0.73      0.25      0.37       133\n",
            "    editorial       0.00      0.00      0.00        58\n",
            "entertainment       0.95      0.84      0.89       237\n",
            "       nation       0.56      0.95      0.70       357\n",
            "       sports       0.00      0.00      0.00        81\n",
            "\n",
            "     accuracy                           0.66       866\n",
            "    macro avg       0.45      0.41      0.39       866\n",
            " weighted avg       0.60      0.66      0.59       866\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Random Forest Model Training and Evaluation"
      ],
      "metadata": {
        "id": "axApkQYSXMVp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Random Forest with FastText\n",
        "rf_classifier = RandomForestClassifier(n_estimators=100)\n",
        "rf_classifier.fit(X_train, y_train)\n",
        "y_pred = rf_classifier.predict(X_test)\n",
        "print(f'\\nRandom Forest with FastText Accuracy: {accuracy_score(y_test, y_pred)}')\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "# Random Forest with TF-IDF\n",
        "rf_classifier.fit(TX_train, Ty_train)\n",
        "Ty_pred = rf_classifier.predict(TX_test)\n",
        "print(f'\\nRandom Forest with TF-IDF Accuracy: {accuracy_score(Ty_test, Ty_pred)}')\n",
        "print(classification_report(Ty_test, Ty_pred))\n",
        "\n",
        "# Random Forest with Word2Vec\n",
        "rf_classifier.fit(WX_train, Wy_train)\n",
        "Wy_pred = rf_classifier.predict(WX_test)\n",
        "print(f'\\nRandom Forest with Word2Vec Accuracy: {accuracy_score(Wy_test, Wy_pred)}')\n",
        "print(classification_report(Wy_test, Wy_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PLAFFq2qXO_i",
        "outputId": "ee670542-8844-4516-ffec-fa0d6f62c10b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Random Forest with FastText Accuracy: 0.9110854503464203\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "entertainment       0.98      0.94      0.96       237\n",
            "       nation       0.91      0.89      0.90       357\n",
            "       others       0.86      0.91      0.88       272\n",
            "\n",
            "     accuracy                           0.91       866\n",
            "    macro avg       0.92      0.91      0.91       866\n",
            " weighted avg       0.91      0.91      0.91       866\n",
            "\n",
            "\n",
            "Random Forest with TF-IDF Accuracy: 0.8175519630484989\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "entertainment       0.89      0.86      0.88       237\n",
            "       nation       0.78      0.87      0.82       357\n",
            "       others       0.81      0.70      0.75       272\n",
            "\n",
            "     accuracy                           0.82       866\n",
            "    macro avg       0.83      0.81      0.82       866\n",
            " weighted avg       0.82      0.82      0.82       866\n",
            "\n",
            "\n",
            "Random Forest with Word2Vec Accuracy: 0.7979214780600462\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "     business       0.69      0.70      0.70       133\n",
            "    editorial       0.62      0.45      0.52        58\n",
            "entertainment       0.95      0.91      0.93       237\n",
            "       nation       0.77      0.85      0.81       357\n",
            "       sports       0.76      0.67      0.71        81\n",
            "\n",
            "     accuracy                           0.80       866\n",
            "    macro avg       0.76      0.71      0.73       866\n",
            " weighted avg       0.80      0.80      0.80       866\n",
            "\n"
          ]
        }
      ]
    }
  ]
}
